{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MSc Neural Network Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../utilities\")\n",
    "import U_config as cfg\n",
    "import os\n",
    "\n",
    "# Data Loader\n",
    "import data_loader\n",
    "\n",
    "# Tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "import visualkeras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, UpSampling2D, concatenate, MaxPooling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# MIT Evidential-Deep Learning Library\n",
    "import evidential_deep_learning as edl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network**\n",
    "\n",
    "Two networks can be created - the traditional U-Net structure (with a modified regressive output), and the 'evidential' network that employs the MIT Evidential Deep Learnning library. The latter is used to generate spatial plots of uncertainty within this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements a modified U-Network, with one less decoder/ encoder block to accomodate the uneven image dimensions that arise when downsampling.\n",
    "def create_model(TOTAL_CHANNELS, evidential):\n",
    "    input = Input((104, 104, TOTAL_CHANNELS))\n",
    "    \n",
    "    c1 = Conv2D(64, 3, activation=\"relu\", padding='same')(input)\n",
    "    c1 = Conv2D(64, 3, activation=\"relu\", padding='same')(c1)\n",
    "    bn1 = BatchNormalization(axis=-1)(c1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(bn1)\n",
    "\n",
    "    c2 = Conv2D(128, 3, activation=\"relu\", padding='same')(pool1)\n",
    "    c2 = Conv2D(128, 3, activation=\"relu\", padding='same')(c2)\n",
    "    bn2 = BatchNormalization(axis=-1)(c2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(bn2)\n",
    "\n",
    "    c3 = Conv2D(256, 3, activation=\"relu\", padding='same')(pool2)\n",
    "    c3 = Conv2D(256, 3, activation=\"relu\", padding='same')(c3)\n",
    "    bn3 = BatchNormalization(axis=-1)(c3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2))(bn3)\n",
    "\n",
    "    c4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    c4 = Conv2D(512, 3, activation='relu', padding='same')(c4)\n",
    "    bn5 = BatchNormalization(axis=-1)(c4)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same')(UpSampling2D(size=(2,2), interpolation='nearest')(bn5))\n",
    "    m7 = concatenate([bn3,up7], axis=3)\n",
    "    c7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(m7)\n",
    "    c7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c7)\n",
    "    bn7 = BatchNormalization(axis=-1)(c7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2), interpolation='nearest')(bn7))\n",
    "    m8 = concatenate([bn2,up8], axis=3)\n",
    "    c8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(m8)\n",
    "    c8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c8)\n",
    "    bn8 = BatchNormalization(axis=-1)(c8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2), interpolation='nearest')(bn8))\n",
    "    m9 = concatenate([c1,up9], axis=3)\n",
    "    c9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(m9)\n",
    "    c9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c9)\n",
    "    c9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c9)\n",
    "\n",
    "    if evidential:\n",
    "        fin = edl.layers.Conv2DNormalGamma(kernel_size=1, filters=1)(c9)\n",
    "        model = Model(input, fin)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=edl.losses.EvidentialRegression)\n",
    "    else:\n",
    "        fin = Conv2D(1,1,activation='sigmoid')(c9)\n",
    "        model = Model(input, fin)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"mean_squared_error\", metrics=\"accuracy\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a data-loader config & produces a Data Loader item for it.\n",
    "def load_data_and_channels(config_file):\n",
    "    cfg = open(config_file)\n",
    "    cfg=json.load(cfg)\n",
    "    data_object = data_loader.ThesisDataLoader(cfg)\n",
    "    channel_count = sum(list(cfg['variables'].values()))\n",
    "    return data_object, channel_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transfer Learning Setup**\n",
    "\n",
    "Pre-Trains the network on the preliminary 1980-2010 dataset (using evidential or traditional model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(EXPERIMENT_ID, experiment_path, destination, evidential, EPOCHS):\n",
    "    \"\"\"\n",
    "        Acquires the model and data loader.\n",
    "        Performs pre training.\n",
    "        Saves incremental improvements of the model.\n",
    "    \"\"\"\n",
    "    if (evidential):\n",
    "        print(\"Training an Evidential Model.\")\n",
    "\n",
    "    # Get data loader\n",
    "    data, channels = load_data_and_channels(f\"./configs/{EXPERIMENT_ID}/pre_training.json\")\n",
    "    \n",
    "    # Get model:\n",
    "    baseline_model = create_model(channels, evidential)\n",
    "    \n",
    "    save_path = os.path.join(experiment_path, f'pre_training_{EXPERIMENT_ID}.h5')\n",
    "\n",
    "    # Setup model callbacks (early stopping & save checkpoints)\n",
    "    es = EarlyStopping(monitor='loss', verbose=1, patience=5)\n",
    "    mc = ModelCheckpoint(destination, monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "    # Pre-train the model:\n",
    "    baseline_model.fit(data, epochs=EPOCHS, verbose=1, callbacks = [es,mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying Weights (Transfer Learning)\n",
    "This block is taken from 'Digital Sreeni':\n",
    "YouTube: \"Tips Tricks 20 - Understanding transfer learning for different size and channel inputs\"\n",
    "https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_tricks_20_Understanding%20transfer%20learning%20for%20different%20size%20and%20channel%20inputs.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE from this block is taken from 'Digital Sreeni':\n",
    "# YouTube: \"Tips Tricks 20 - Understanding transfer learning for different size and channel inputs\"\n",
    "# https://github.com/bnsreenu/python_for_microscopists/blob/master/Tips_tricks_20_Understanding%20transfer%20learning%20for%20different%20size%20and%20channel%20inputs.py\n",
    "def copy_weights(weights, num_channels_to_fill):  \n",
    "  average_weights = np.mean(weights, axis=-2).reshape(weights[:,:,-1:,:].shape)  \n",
    "  wts_copied_to_mult_channels = np.tile(average_weights, (num_channels_to_fill, 1)) \n",
    "  return(wts_copied_to_mult_channels)\n",
    "\n",
    "def create_transfer_network(source, evidential=False):\n",
    "    data, new_channels = load_data_and_channels(f\"./configs/{EXPERIMENT_ID}/post_training.json\")\n",
    "    old_channel_count = sum(list(json.load(open(f\"./configs/{EXPERIMENT_ID}/pre_training.json\"))['variables'].values()))\n",
    "\n",
    "    print(f'\\nThe old model had {old_channel_count} channels, the new model has {new_channels}.\\n')\n",
    "\n",
    "    # Get setup:\n",
    "\n",
    "    if evidential:\n",
    "        pre_trained_model = tf.keras.models.load_model(source, custom_objects={'Conv2DNormalGamma': edl.layers.Conv2DNormalGamma, 'EvidentialRegression':edl.losses.EvidentialRegression})\n",
    "    else:\n",
    "       pre_trained_model = tf.keras.models.load_model(source)\n",
    "    pre_model_config = pre_trained_model.get_config()\n",
    "    pre_model_config[\"layers\"][0][\"config\"][\"batch_input_shape\"] = (None, 104, 104, new_channels)\n",
    "\n",
    "    # Load the post-training model:\n",
    "    if evidential:\n",
    "        post_model = tf.keras.Model.from_config(pre_model_config, custom_objects={'Conv2DNormalGamma': edl.layers.Conv2DNormalGamma, 'EvidentialRegression':edl.losses.EvidentialRegression})\n",
    "    else:\n",
    "        post_model = tf.keras.Model.from_config(pre_model_config)\n",
    "    \n",
    "    post_model_config = post_model.get_config()\n",
    "    post_model_layers = [post_model_config['layers'][x]['name'] for x in range(len(post_model_config['layers']))]\n",
    "    conv_layer = post_model_layers[1]\n",
    "\n",
    "    # Iterate across & copy weights:\n",
    "    for layer in pre_trained_model.layers:\n",
    "        if layer.name in post_model_layers:\n",
    "            if (layer.get_weights() != []): \n",
    "                target_layer = post_model.get_layer(layer.name)\n",
    "                if layer.name == conv_layer:\n",
    "                    weights = layer.get_weights()[0]\n",
    "                    biases = layer.get_weights()[1]                    \n",
    "                    weights_extra_channels = copy_weights(weights, new_channels - old_channel_count)\n",
    "                    new_weights = np.concatenate((weights, weights_extra_channels), axis=-2)\n",
    "                    target_layer.set_weights([new_weights, biases])                    \n",
    "                    target_layer.trainable == True\n",
    "                else:\n",
    "                    target_layer.set_weights(layer.get_weights())\n",
    "                    target_layer.trainable=True\n",
    "    \n",
    "    del pre_trained_model\n",
    "\n",
    "    if evidential:\n",
    "        post_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=edl.losses.EvidentialRegression)\n",
    "    else:\n",
    "        post_model.compile(optimizer=Adam(), loss=\"mean_squared_error\", metrics=\"accuracy\")\n",
    "    return post_model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_train(EXPERIMENT_ID, pre_path, destination, evidential, EPOCHS):\n",
    "    \"\"\"\n",
    "    Sets-up the model and data loader for POST-Training (2011-2018)\n",
    "    Proceeds to train the network, saving incremental improvements (based on the loss).\n",
    "    \"\"\"\n",
    "    post_model, data = create_transfer_network(pre_path, evidential)\n",
    "    es = EarlyStopping(monitor='loss', verbose=1, min_delta=0.000001, patience=3)\n",
    "    mc = ModelCheckpoint(destination, monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "    post_model.fit(data, epochs=EPOCHS, verbose=1, callbacks = [es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_train(EXPERIMENT_ID, pre_path, destination, evidential, EPOCHS):\n",
    "    \"\"\"\n",
    "    Sets-up the model and data loader for post training using ERA-5 and SIC data only (2011-2018). Consequently, the number of channels remains the same.\n",
    "    \"\"\"\n",
    "    data, channels = load_data_and_channels(f\"./configs/{EXPERIMENT_ID}/post_training_baseline.json\")\n",
    "        \n",
    "    if evidential:\n",
    "        baseline_model = tf.keras.models.load_model(pre_path, custom_objects={'Conv2DNormalGamma': edl.layers.Conv2DNormalGamma, 'EvidentialRegression':edl.losses.EvidentialRegression})\n",
    "        baseline_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=edl.losses.EvidentialRegression)\n",
    "    else:\n",
    "        baseline_model = tf.keras.models.load_model(pre_path)\n",
    "    \n",
    "    es = EarlyStopping(monitor='loss', verbose=1, patience=5)\n",
    "    mc = ModelCheckpoint(destination, monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "    baseline_model.fit(data, epochs=EPOCHS, verbose=1, callbacks = [es,mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT CONFIGURATION\n",
    "EXPERIMENT_ID = 401\n",
    "EPOCHS = 15\n",
    "EVIDENTIAL = True\n",
    "experiment_path = os.path.join(cfg.MODELS_FOLDER, 'models', str(EXPERIMENT_ID))\n",
    "\n",
    "# SETUP EXPERIMENT FILE STRUCTURE\n",
    "if not os.path.isfile(experiment_path):\n",
    "    os.makedirs(experiment_path)\n",
    "\n",
    "# ================= PRE-TRAINING =================\n",
    "#Setup & train the pre-trained network.\n",
    "pre_train_path = os.path.join(experiment_path, f'pre_training_{EXPERIMENT_ID}.h5')\n",
    "pre_train(EXPERIMENT_ID, experiment_path, pre_train_path, EVIDENTIAL, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Post-Training, \"Data-Rich\" Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 401\n",
    "EPOCHS = 15\n",
    "\n",
    "# ================= POST-TRAINING, ADDITIONAL VARIABLES =================\n",
    "experiment_path = os.path.join(cfg.MODELS_FOLDER, 'models', str(EXPERIMENT_ID))\n",
    "pre_train_path = os.path.join(experiment_path, f'pre_training_{EXPERIMENT_ID}.h5')\n",
    "post_train_path = os.path.join(experiment_path, f'post_training_{EXPERIMENT_ID}.h5')\n",
    "post_train(EXPERIMENT_ID, pre_train_path, post_train_path, True, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Post-Training, Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 401\n",
    "EPOCHS = 15\n",
    "\n",
    "# ================= POST-TRAINING, NO ADDITIONAL VARIABLES =================\n",
    "experiment_path = os.path.join(cfg.MODELS_FOLDER, 'models', str(EXPERIMENT_ID))\n",
    "pre_trained_path = os.path.join(experiment_path, f'pre_training_{EXPERIMENT_ID}.h5')\n",
    "baseline_path = os.path.join(experiment_path, f'post_baseline_{EXPERIMENT_ID}.h5')\n",
    "baseline_train(EXPERIMENT_ID, pre_trained_path, baseline_path, True, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(source):\n",
    "    # COMMENT OUT BASED ON IF THE EVIDENTIAL OR TRADITIONAL MODEL IS USED!\n",
    "    model = tf.keras.models.load_model(source, custom_objects={'Conv2DNormalGamma': edl.layers.Conv2DNormalGamma, 'EvidentialRegression':edl.losses.EvidentialRegression})\n",
    "    #model = tf.keras.models.load_model(source)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_array(sample):\n",
    "    sample = np.moveaxis(sample, 2,3)\n",
    "    sample = np.moveaxis(sample, 1,2)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_evidenced_predictions(data_loader, model, save_location):    \n",
    "    # Save the results (deleting previous results):    \n",
    "    if not os.path.exists(save_location):\n",
    "        os.makedirs(save_location)\n",
    "    \n",
    "    X,Y = data_loader.__getitem__(0)\n",
    "    print(data_loader.dates)\n",
    "    outputs = model.predict(X)\n",
    "\n",
    "    # ========== MODEL ESTIMATES (MU) ==========\n",
    "    estimates = reshape_array(outputs)\n",
    "    estimates_loc = os.path.join(save_location, \"non_evidenced_estimates.npy\")\n",
    "    np.save(estimates_loc, estimates)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidential Regression:\n",
    "- The network learns a number of parameters during evidential regression which are subsequently used to determine error, mean guess etc.\n",
    "- https://github.com/aamini/evidential-deep-learning\n",
    "\n",
    "`import evidential_deep_learning as edl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(data_loader, model, save_location):\n",
    "    X,Y = data_loader.__getitem__(0)\n",
    "    outputs = model.predict(X)\n",
    "    mu, v, alpha, beta = tf.split(outputs, 4, axis=-1)\n",
    "    \n",
    "    # ========== MODEL ESTIMATES (MU) ==========\n",
    "    estimates = reshape_array(np.clip(mu, 0, 1))\n",
    "    estimates_loc = os.path.join(save_location, \"estimates.npy\")\n",
    "\n",
    "    # ========== EPISTEMIC UNCERTAINTY (VARIANCE OF MU) ==========\n",
    "    epistemic = reshape_array(tf.sqrt(beta/(v*(alpha-1))))\n",
    "    epistemic_loc = os.path.join(save_location, \"epistemic.npy\")\n",
    "\n",
    "    # ========== ALEATORIC UNCERTAINTY (...) ==========\n",
    "    aleatoric = reshape_array(tf.sqrt(beta/(alpha-1)))\n",
    "    aleatoric_loc = os.path.join(save_location, \"aleatoric.npy\")\n",
    "\n",
    "    # ============ VARIANCE ==================\n",
    "    variance_loc = os.path.join(save_location, \"variance.npy\")\n",
    "\n",
    "    # ========== GROUND TRUTH (SIC [Y]) ==========\n",
    "    Y = reshape_array(Y)\n",
    "    Y_loc = os.path.join(save_location, \"truth.npy\")\n",
    "\n",
    "    # Save the results (deleting previous results):    \n",
    "    if not os.path.exists(save_location):\n",
    "        os.makedirs(save_location)\n",
    "\n",
    "    np.save(estimates_loc, estimates)\n",
    "    np.save(epistemic_loc, epistemic)\n",
    "    np.save(aleatoric_loc, aleatoric)\n",
    "    np.save(variance_loc, v)\n",
    "    np.save(Y_loc, Y)\n",
    "    return outputs, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_predict(type, test_config_name):\n",
    "    \"\"\"\n",
    "    Loads a specific model & generates predictions for a given input by making a Data Loader from a specified configuration file\n",
    "    \"\"\"\n",
    "    path = os.path.join(experiment_path, f'{type}_{EXPERIMENT_ID}.h5')\n",
    "    model = load_model(path)\n",
    "    test_data, new_channels = load_data_and_channels(f\"./configs/{EXPERIMENT_ID}/{test_config_name}.json\")\n",
    "    del new_channels\n",
    "    save_loc = os.path.join(experiment_path, type)\n",
    "    return generate_predictions(test_data, model, save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = 401\n",
    "experiment_path = os.path.join(cfg.MODELS_FOLDER, 'models', str(EXPERIMENT_ID))\n",
    "load_and_predict('post_baseline', 'baseline_testing')\n",
    "load_and_predict('post_training', 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variable Importance Analysis**\n",
    "Determines the error induced by assigning 0 to variables in turn (both in regards to extent and RMSE scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap_dict(EXPERIMENT_ID, type):\n",
    "    \n",
    "    cfg = open(f\"./configs/{EXPERIMENT_ID}/testing.json\")\n",
    "    cfg=json.load(cfg)\n",
    "    variables = [\"sic\", \"2m_temperature\", \"sea_surface_temperature\", \"mean_sea_level_pressure\", \"surface_solar_radiation_upwards\", \"surface_net_solar_radiation\", \"10m_v_component_of_wind\", \"sit\", \"u_drift\", \"v_drift\"]\n",
    "    path = os.path.join(f\"./models/{EXPERIMENT_ID}/post_training_{EXPERIMENT_ID}.h5\")\n",
    "    model = tf.keras.models.load_model(path)\n",
    "\n",
    "    new_metrics = []\n",
    "    for var in variables:\n",
    "        # Setup the new model and the dataloader:\n",
    "        cfg[\"permute_var\"] = var\n",
    "        data_object = data_loader_perm.ThesisDataLoader(cfg)\n",
    "        X,Y = data_object.__getitem__(0)\n",
    "        outputs = model.predict(X)\n",
    "        estimates = reshape_array(outputs)\n",
    "        np.save(f\"./models/{EXPERIMENT_ID}/var_importance{var}.npy\", estimates)\n",
    "        \n",
    "        if type == \"extent\":\n",
    "            new_metrics.append(spatial_extent(estimates))\n",
    "        else:\n",
    "            new_metrics.append(generate_RMSE(estimates))\n",
    "\n",
    "    # Get the original metrics:\n",
    "    cfg[\"permute_var\"] = None\n",
    "    data_object = data_loader_perm.ThesisDataLoader(cfg)\n",
    "    X,Y = data_object.__getitem__(0)\n",
    "    outputs = model.predict(X)\n",
    "    estimates = reshape_array(outputs)\n",
    "\n",
    "    if type == \"extent\":\n",
    "        original_metrics = spatial_extent(estimates)\n",
    "    else:\n",
    "        original_metrics = generate_RMSE(estimates)\n",
    "\n",
    "    variables = [\"SIC\", \"2m Temperature\", \"SST\", \"Mean Sea Level Pressure\", \"Surface Solar Radiation (Down)\", \"Surface Solar Radiation (Net)\", \"10m Wind (v)\", \"SIT\", \"Drift (u)\", \"Drift (v)\"]\n",
    "    var_importance = {}\n",
    "\n",
    "    count=0\n",
    "    for key in variables:\n",
    "        if type == \"extent\": \n",
    "            var_importance[key] = original_metrics[count]-new_metrics[count]\n",
    "        else:\n",
    "            var_importance[key] = new_metrics[count]/original_metrics[count]\n",
    "        count+=1\n",
    "\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    var_imp = pd.DataFrame(var_importance)\n",
    "    var_imp = var_imp.rename({i:months[i] for i in range(0,12)})\n",
    "    return var_imp.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,(ax1,ax2,ax3, axcb) = plt.subplots(1,4,  gridspec_kw={'width_ratios':[2.5,2.5,2.5,0.2]}, figsize=(12,4))\n",
    "ax1.get_shared_y_axes().join(ax2,ax3)\n",
    "\n",
    "g1 = sns.heatmap(month_1_extent,cmap=\"RdBu\",cbar=False,ax=ax1)\n",
    "g1.set_ylabel('')\n",
    "g1.set_xlabel('')\n",
    "g1.set_title(\"1 Month\")\n",
    "g2 = sns.heatmap(month_2_extent,cmap=\"RdBu\",cbar=False,ax=ax2)\n",
    "g2.set_ylabel('')\n",
    "g2.set_xlabel('')\n",
    "g2.set_yticks([])\n",
    "g2.set_title(\"2 Month\")\n",
    "g3 = sns.heatmap(month_3_extent,cmap=\"RdBu\",ax=ax3, cbar_ax=axcb)\n",
    "g3.set_ylabel('')\n",
    "g3.set_xlabel('')\n",
    "g3.set_title(\"3 Month\")\n",
    "g3.set_yticks([])\n",
    "\n",
    "plt.suptitle(\"Variable Sensitivity for Sea Ice Extent\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "NSIDC = load_year(YEAR)\n",
    "mask = NSIDC[0]\n",
    "    \n",
    "def spatial_extent(estimates):\n",
    "    thickness = []\n",
    "    nsidc = []\n",
    "    ground_truth = np.load(\"../data/processed_data/sic/2010_01.npy\")\n",
    "        \n",
    "    for i in range(0, 12):\n",
    "        mean_thick = pre_process_variable(estimates[i][0], ground_truth)\n",
    "        mask = np.zeros((100, 100))\n",
    "        mask[np.where(mean_thick>0.15)] = 1\n",
    "        thickness.append(np.sum(mask)*80/10000)\n",
    "    return thickness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximums = [\"2012_01.npy\", \"2013_02.npy\", \"2013_03.npy\", \"2010_04.npy\", \"2010_05.npy\", \"2013_06.npy\", \"2013_07.npy\", \"2014_08.npy\", \"2014_09.npy\", \"2013_10.npy\", \"2014_11.npy\", \"2014_12.npy\"]\n",
    "sic_path = \"../data/datasets/post_standard/sic/\"\n",
    "def mask_maximum(sample, ground_truth, maximum):\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= 0\n",
    "    maximum_sic = np.load(sic_path+maximum)\n",
    "    mask = np.zeros((104, 104))\n",
    "    mask[np.where(maximum_sic>0.15)]=1\n",
    "    sample = sample*mask\n",
    "    return sample[:100, :100]*100\n",
    "\n",
    "def generate_RMSE(estimates):\n",
    "    ground_truth = np.load(\"../data/processed_data/sic/2010_01.npy\")\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    RMSE = []\n",
    "    \n",
    "    for m in range(0, 12):\n",
    "        print(months[m])\n",
    "        ice_sample = mask_maximum(estimates[0][0], ground_truth, maximums[m])\n",
    "        if m <9:\n",
    "            month = f'0{m+1}'\n",
    "        else:\n",
    "            month = f'{m+1}'\n",
    "        true_ice = mask_maximum(np.load(f'../data/datasets/post_standard/sic/2019_{month}.npy'), ground_truth, maximums[m])\n",
    "        RMSE.append(mean_squared_error(true_ice, ice_sample, squared = False))\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GENERAL ERROR METRICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# =========== CHANGE ===========\n",
    "EXPERIMENT_ID = 302\n",
    "# ===============================\n",
    "\n",
    "maximums = [\"2012_01.npy\", \"2013_02.npy\", \"2013_03.npy\", \"2010_04.npy\", \"2010_05.npy\", \"2013_06.npy\", \"2013_07.npy\", \"2014_08.npy\", \"2014_09.npy\", \"2013_10.npy\", \"2014_11.npy\", \"2014_12.npy\"]\n",
    "sic_path = \"../data/datasets/post_standard/sic/\"\n",
    "\n",
    "def mask_maximum(sample, ground_truth, maximum):\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= 0\n",
    "    maximum_sic = np.load(sic_path+maximum)\n",
    "    mask = np.zeros((104, 104))\n",
    "    mask[np.where(maximum_sic>0.15)]=1\n",
    "    sample = sample*mask\n",
    "    return sample[:100, :100]*100\n",
    "\n",
    "def generate_RMSE(estimates):\n",
    "    ground_truth = np.load(\"../data/processed_data/sic/2010_01.npy\")\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    RMSE = []\n",
    "    \n",
    "    for m in range(0, 12):\n",
    "        ice_sample = mask_maximum(estimates[0][0], ground_truth, maximums[m])\n",
    "        if m <9:\n",
    "            month = f'0{m+1}'\n",
    "        else:\n",
    "            month = f'{m+1}'\n",
    "        true_ice = mask_maximum(np.load(f'../data/datasets/post_standard/sic/2019_{month}.npy'), ground_truth, maximums[m])\n",
    "        RMSE.append(mean_squared_error(true_ice, ice_sample, squared = False))\n",
    "    return RMSE\n",
    "\n",
    "def generate_error_metrics(estimates):\n",
    "    ground_truth = np.load(\"../data/processed_data/sic/2010_01.npy\")\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    RMSE = []\n",
    "    NRMSE = []\n",
    "    NSE = []\n",
    "    MAE = []\n",
    "\n",
    "    for m in range(0, 12):\n",
    "        ice_sample = mask_maximum(estimates[0][0], ground_truth, maximums[m])\n",
    "        if m <9:\n",
    "            month = f'0{m+1}'\n",
    "        else:\n",
    "            month = f'{m+1}'\n",
    "\n",
    "        true_ice = mask_maximum(np.load(f'../data/datasets/post_standard/sic/2019_{month}.npy'), ground_truth, maximums[m])\n",
    "\n",
    "         # CALCULATE RMSE:\n",
    "        RMSE.append(mean_squared_error(true_ice, ice_sample, squared = False))\n",
    "\n",
    "        # CALCULATE nRMSE:\n",
    "        NRMSE.append(RMSE[m-1]/np.std(true_ice))\n",
    "\n",
    "        # CALCULATE MAE:\n",
    "        MAE.append(mean_absolute_error(true_ice, ice_sample))\n",
    "\n",
    "        # CALCULATE NSE:\n",
    "        NSE.append((1-(np.sum((true_ice-ice_sample)**2)/np.sum((true_ice-np.mean(true_ice))**2))))\n",
    "    \n",
    "    print(RMSE)\n",
    "    print(\"=================================================================================================================\")\n",
    "    print(f'|\\tMonth\\t|\\tRMSE\\t\\t|\\tnRMSE\\t\\t|\\tMAE\\t\\t|\\tNSE\\t\\t|')\n",
    "    print(\"=================================================================================================================\")\n",
    "    for month_count, month in enumerate(months):\n",
    "        print(f'|\\t{month}\\t|\\t{RMSE[month_count-1]:.3f}\\t\\t|\\t{NRMSE[month_count-1]:.3f}\\t\\t|\\t{MAE[month_count-1]:.3f}\\t\\t|\\t{NSE[month_count-1]:.3f}\\t\\t|')\n",
    "    print(\"=================================================================================================================\")\n",
    "    # APPLY JULIANs METRICS?\n",
    "\n",
    "    #print(np.mean(np.array(MAE)))\n",
    "\n",
    "    return \n",
    "\n",
    "thickness_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_training/non_evidenced_estimates.npy\")\n",
    "baseline_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/non_evidenced_estimates.npy\")\n",
    "print(\"BASELINE\")\n",
    "generate_error_metrics(baseline_predictions)\n",
    "print(\"THICKNESS\")\n",
    "generate_error_metrics(thickness_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot Results** \n",
    "### **Requires Kernel Restart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../utilities\")\n",
    "import U_config as cfg\n",
    "import U_plots as p\n",
    "\n",
    "# MODIFY THESE\n",
    "YEAR = 2019\n",
    "EXPERIMENT_ID =401\n",
    "TYPE = \"non_evidenced_estimates\"\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "DATES = [f\"{month} {YEAR}\" for month in MONTHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_year(year):\n",
    "    truth = []\n",
    "    for i in range(1, 13):\n",
    "        truth.append(np.load(f\"../data/processed_data/sic/{year}_{f'0{i}' if i<10 else i}.npy\"))\n",
    "    return np.array(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_variable(sample, ground_truth):\n",
    "    sample[sample>1] = 1\n",
    "    sample[sample<0] = 0\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= np.nan\n",
    "    return sample[:100, :100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_thick(sample, ground_truth):\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= np.nan\n",
    "    return sample[:100, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximums = [\"2012_01.npy\", \"2013_02.npy\", \"2013_03.npy\", \"2010_04.npy\", \"2010_05.npy\", \"2013_06.npy\", \"2013_07.npy\", \"2014_08.npy\", \"2014_09.npy\", \"2013_10.npy\", \"2014_11.npy\", \"2014_12.npy\"]\n",
    "sic_path = \"../data/datasets/post_standard/sic/\"\n",
    "\n",
    "def mask_non_active_region(sample, ground_truth, maximum):\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= np.nan\n",
    "    maximum_sic = np.load(sic_path+maximum)\n",
    "    sample[np.where(maximum_sic<0.15)]=np.nan\n",
    "    return sample[:100, :100]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Main Method for Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Evidenced Results\n",
    "plot_object = p.plots()\n",
    "NSIDC = load_year(YEAR)\n",
    "mask = NSIDC[0]\n",
    "EXPERIMENT_ID = 401\n",
    "\n",
    "LAG_MONTHS = [\"November\", \"December\", \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\"]\n",
    "\n",
    "#thickness_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_training/non_evidenced_estimates.npy\")\n",
    "#baseline_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/non_evidenced_estimates.npy\")\n",
    "#thickness_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_training/estimates.npy\")\n",
    "#baseline_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/estimates.npy\")\n",
    "\n",
    "thickness_epi = np.load(f\"./models/{EXPERIMENT_ID}/post_training/epistemic.npy\")\n",
    "baseline_epi = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/epistemic.npy\")\n",
    "\n",
    "#thickness_aleo = np.load(f\"./models/{EXPERIMENT_ID}/post_training/aleatoric.npy\")\n",
    "#baseline_aleo = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/aleatoric.npy\")\n",
    "# thickness_plots = f\"./models/{EXPERIMENT_ID}/post_training/\"\n",
    "# baseline_plots = f\"./models/{EXPERIMENT_ID}/post_baseline/\"\n",
    "# comparitive_plots = f\"./models/{EXPERIMENT_ID}/comparative\"\n",
    "\n",
    "# plotting uncertainty;\n",
    "for i in range(4, 12):\n",
    "    thick = mask_non_active_region(thickness_epi[i][0], mask, maximums[i])\n",
    "    base = mask_non_active_region(baseline_epi[i][0], mask, maximums[i])\n",
    "    plot_object.plot_uncertainty(NSIDC[i], thick, base, DATES[i], comparitive_plots+f\"/{DATES[i]}_epistemic.png\", 0, 3, \"plasma\", \"Epistemic Uncertainty of the Observational Model\", \"Epistemic Uncertainty of the Baseline Model\", contour=False)\n",
    "\n",
    "    #a_thick = mask_non_active_region(thickness_aleo[i][0], mask, maximums[i])\n",
    "    #a_base = mask_non_active_region(baseline_aleo[i][0], mask, maximums[i])\n",
    "    #plot_object.plot_uncertainty(NSIDC[i], a_thick, a_base, DATES[i], comparitive_plots+f\"/{DATES[i]}_aleatoric.png\", 0, 3, \"plasma\", \"Aleatoric Uncertainty of the Observational Model\", \"Aleatoric Uncertainty of the Baseline Model\", contour=False)\n",
    "    \n",
    "    #thick = pre_process_variable(thickness_predictions[i][0], mask)\n",
    "    #base = pre_process_variable(baseline_predictions[i][0], mask)\n",
    "    #plot_object.plot_prediction(NSIDC[i]*100, thick*100, base*100, DATES[i], comparitive_plots+f\"/{DATES[i]}_estimates.png\", contour=False)\n",
    "    \n",
    "    #plot_object.plot_anom(NSIDC[i]*100, thick*100, base*100, DATES[i], comparitive_plots+f\"/{DATES[i]}_anomalies.png\", col='#5c5c5c', contour=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Evidenced Results\n",
    "plot_object = p.plots()\n",
    "\n",
    "NSIDC = load_year(YEAR)\n",
    "mask = NSIDC[0]\n",
    "EXPERIMENT_ID = 303\n",
    "LAG_MONTHS = [\"November\", \"December\", \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\"]\n",
    "\n",
    "thickness_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_training/non_evidenced_estimates.npy\")\n",
    "baseline_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/non_evidenced_estimates.npy\")\n",
    "#thickness_data = [np.load(f'../data/processed_data/sit/2018_12.npy'), np.load(f'../data/processed_data/sit/2019_01.npy'), np.load(f'../data/processed_data/sit/2019_02.npy'), np.load(f'../data/processed_data/sit/2019_03.npy')\n",
    "#                  , np.load(f'../data/processed_data/sit/2019_04.npy'), np.load(f'../data/processed_data/sit/2019_05.npy'), np.load(f'../data/processed_data/sit/2019_06.npy'), np.load(f'../data/processed_data/sit/2019_07.npy')\n",
    "#                  , np.load(f'../data/processed_data/sit/2019_08.npy'), np.load(f'../data/processed_data/sit/2019_09.npy'), np.load(f'../data/processed_data/sit/2019_10.npy'), np.load(f'../data/processed_data/sit/2019_11.npy')]\n",
    "\n",
    "thickness_plots = f\"./models/{EXPERIMENT_ID}/post_training/\"\n",
    "baseline_plots = f\"./models/{EXPERIMENT_ID}/post_baseline/\"\n",
    "comparitive_plots = f\"./models/{EXPERIMENT_ID}/comparative\"\n",
    "\n",
    "for i in range(0, 12):\n",
    "    thick = pre_process_variable(thickness_predictions[i][0], mask)\n",
    "    base = pre_process_variable(baseline_predictions[i][0], mask)\n",
    "    #thickness = pre_process_thick(thickness_data[i], mask)\n",
    "    plot_object.plot_prediction(NSIDC[i]*100, thick*100, base*100, DATES[i], comparitive_plots+f\"/{DATES[i]}_estimates.png\", contour=False)\n",
    "    plot_object.plot_anom(NSIDC[i]*100, thick*100, base*100, DATES[i], comparitive_plots+f\"/{DATES[i]}_anomalies.png\", col='#5c5c5c', contour=False)\n",
    "    #plot_object.plot_anom_thick(NSIDC[i]*100, thick*100, base*100, thickness, LAG_MONTHS[i], DATES[i], 0, 5, 1, comparitive_plots+f\"/{DATES[i]}_anomalies_thick.png\", col='#5c5c5c', contour=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "NSIDC = load_year(YEAR)\n",
    "mask = NSIDC[0]\n",
    "\n",
    "def pre_process_variable(sample, ground_truth):\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= 0\n",
    "    sample[sample>1] = 1\n",
    "    sample[sample<0] = 0\n",
    "    return sample[:100, :100]\n",
    "\n",
    "def spatial_extent(EXPERIMENT_ID):\n",
    "    \n",
    "    thickness_mu = np.load(f\"./models/{EXPERIMENT_ID}/post_training/non_evidenced_estimates.npy\")\n",
    "    baseline_mu = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/non_evidenced_estimates.npy\")\n",
    "\n",
    "    sic_data = [np.load(f'../data/processed_data/sic/2019_01.npy'), np.load(f'../data/processed_data/sic/2019_02.npy'), np.load(f'../data/processed_data/sic/2019_03.npy')\n",
    "                  , np.load(f'../data/processed_data/sic/2019_04.npy'), np.load(f'../data/processed_data/sic/2019_05.npy'), np.load(f'../data/processed_data/sic/2019_06.npy'), np.load(f'../data/processed_data/sic/2019_07.npy')\n",
    "                  , np.load(f'../data/processed_data/sic/2019_08.npy'), np.load(f'../data/processed_data/sic/2019_09.npy'), np.load(f'../data/processed_data/sic/2019_10.npy'), np.load(f'../data/processed_data/sic/2019_11.npy'), np.load(f'../data/processed_data/sic/2019_12.npy')]\n",
    "\n",
    "    ground_truth = np.load(\"../data/processed_data/sic/2010_01.npy\")\n",
    "\n",
    "    obs_res = []\n",
    "    base_res = []  \n",
    "    nsidc = []\n",
    "\n",
    "    for i in range(0, 12):\n",
    "\n",
    "        thickness = pre_process_variable(thickness_mu[i][0], ground_truth)\n",
    "        mask = np.zeros((100, 100))\n",
    "        mask[np.where(thickness>0.15)] = 1\n",
    "        obs_res.append(np.sum(mask)*80/10000)\n",
    "        \n",
    "        baseline = pre_process_variable(baseline_mu[i][0], ground_truth)\n",
    "        mask = np.zeros((100, 100))\n",
    "        mask[np.where(baseline>0.15)] = 1\n",
    "        base_res.append(np.sum(mask)*80/10000)\n",
    "        \n",
    "        sic = sic_data[i]\n",
    "        mask = np.zeros((100, 100))\n",
    "        mask[np.where(sic>0.15)] = 1\n",
    "        nsidc.append(np.sum(mask)*80/10000)\n",
    "\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    print(\"============================================================================================================================\")\n",
    "    print(\"|\\tMonth\\t|\\tNSIDC\\t|\\tBaseline Extent\\t|\\tDelta\\t|\\tThickness Extent\\t|\\tDelta\\t|\")\n",
    "    print(\"============================================================================================================================\")\n",
    "    \n",
    "    for i in range(0, 12):\n",
    "        print(f'|\\t{months[i]}\\t|\\t{nsidc[i]}\\t|\\t{base_res[i]}\\t\\t|\\t{nsidc[i]-base_res[i]:.3f}\\t|\\t{obs_res[i]}\\t\\t\\t|\\t{nsidc[i]-obs_res[i]:.3f}\\t|')\n",
    "    print(\"============================================================================================================================\")\n",
    "    \n",
    "    return base_res, obs_res, nsidc\n",
    "\n",
    "baseline, thickness, nsidc = spatial_extent(302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "fig, (ax1) = plt.subplots(1,1, figsize=(12, 4), constrained_layout=True)\n",
    "\n",
    "ax1.plot(months, baseline,  label='Baseline Extent', color='#1cb0ff')\n",
    "ax1.plot(months, thickness,  label='Thickness Extent', color='#ff491c')\n",
    "ax1.plot(months, nsidc,  label='NSIDC Extent', color='gray', linestyle='--')\n",
    "\n",
    "plt.suptitle(f\"Comparison of SIE (Millions 10^3 km^3)\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "f = plt.figure(figsize=[10,4])\n",
    "ax = plt.axes()\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", ]\n",
    "\n",
    "# Lags : \n",
    "#january_b = baseline[0]\n",
    "#baseline.pop(0)\n",
    "#baseline.append(january_b)\n",
    "#january_b = thickness[0]\n",
    "#thickness.pop(0)\n",
    "#thickness.append(january_b)\n",
    "\n",
    "plt.plot(months, baseline,  label='Baseline Model', color='#1cb0ff')\n",
    "plt.plot(months, thickness, label='Observational Model', color='#ff491c')\n",
    "plt.plot(months, nsidc, linestyle='dashed', label='NSIDC', color='#595959')\n",
    "plt.axis('tight')\n",
    "plt.title(\"Monthly Averaged Sea-Ice Extent for 1-Month Forecasts (Millions Km^2)\\nLAGGED FORECAST\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Extent in Millions Km^2\")\n",
    "legend=plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "maximums = [\"2012_01.npy\", \"2013_02.npy\", \"2013_03.npy\", \"2010_04.npy\", \"2010_05.npy\", \"2013_06.npy\", \"2013_07.npy\", \"2014_08.npy\", \"2014_09.npy\", \"2013_10.npy\", \"2014_11.npy\", \"2014_12.npy\"]\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", ]\n",
    "sic_path = \"../data/datasets/post_standard/sic/\"\n",
    "\n",
    "def mask_maximum(sample, ground_truth, maximum):\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= 0\n",
    "    maximum_sic = np.load(sic_path+maximum)\n",
    "    mask = np.zeros((104, 104))\n",
    "    mask[np.where(maximum_sic>0.15)]=1\n",
    "    sample = sample*mask\n",
    "    return sample[:100, :100]*100\n",
    "\n",
    "\n",
    "def SIE_histogram(experiment_ID, path):\n",
    "    thickness_predictions = np.load(f\"./models/{experiment_ID}/post_training/non_evidenced_estimates.npy\")\n",
    "    baseline_predictions = np.load(f\"./models/{experiment_ID}/post_baseline/non_evidenced_estimates.npy\")\n",
    "    sic_data = [np.load(f'../data/processed_data/sic/2019_01.npy'), np.load(f'../data/processed_data/sic/2019_02.npy'), np.load(f'../data/processed_data/sic/2019_03.npy')\n",
    "                  , np.load(f'../data/processed_data/sic/2019_04.npy'), np.load(f'../data/processed_data/sic/2019_05.npy'), np.load(f'../data/processed_data/sic/2019_06.npy'), np.load(f'../data/processed_data/sic/2019_07.npy')\n",
    "                  , np.load(f'../data/processed_data/sic/2019_08.npy'), np.load(f'../data/processed_data/sic/2019_09.npy'), np.load(f'../data/processed_data/sic/2019_10.npy'), np.load(f'../data/processed_data/sic/2019_11.npy'), np.load(f'../data/processed_data/sic/2019_12.npy')]\n",
    "\n",
    "    ground_truth = np.load(\"../data/processed_data/sic/2010_01.npy\")        \n",
    "    \n",
    "    for i in range(0, 12):\n",
    "        f, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 3))\n",
    "        month = f'0{i+1}' if i <9 else f'{i+1}'\n",
    "\n",
    "        sic = mask_maximum(np.load(f'../data/datasets/post_standard/sic/2019_{month}.npy'), ground_truth, maximums[i])\n",
    "        ax2.hist(sic.ravel(), bins=np.arange(0, 101, 2),  color='#58595c', log=True, edgecolor = \"black\", label=\"NSIDC\")\n",
    "        ax2.set_title(\"Baseline Model\")\n",
    "        \n",
    "        base = mask_maximum(baseline_predictions[i][0], ground_truth, maximums[i])\n",
    "        ax2.hist(base.ravel(), bins=np.arange(0, 101, 2),  color='#3882c2', log=True, edgecolor = \"#0d416e\", alpha=0.5, label=\"Baseline\")\n",
    "        ax2.legend()\n",
    "        \n",
    "        ax1.hist(sic.ravel(), bins=np.arange(0, 101, 2),  color='#58595c', log=True, edgecolor = \"black\", label=\"NSIDC\")\n",
    "        ax1.set_title(\"Observational Model\")\n",
    "        thick = mask_maximum(thickness_predictions[i][0], ground_truth, maximums[i])\n",
    "        ax1.hist(thick.ravel(), bins=np.arange(0, 101, 2),  color='#f59042', log=True, edgecolor = \"#c2590a\", alpha=0.5, label=\"Observational\")\n",
    "        ax1.legend()\n",
    "\n",
    "        plt.suptitle(f\"Comparison of SIC Distribution relative to NSIDC between Models for {months[i]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path+f\"_{months[i]}.png\")\n",
    "        plt.show()\n",
    "\n",
    "#https://stackoverflow.com/questions/64156602/how-to-build-a-histogram-of-numpy-2-dimensional-array\n",
    "#https://stackoverflow.com/questions/64156602/how-to-build-a-histogram-of-numpy-2-dimensional-array\n",
    "\n",
    "SIE_histogram(303, \"./models/303/\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# =========== CHANGE ===========\n",
    "EXPERIMENT_ID = 301\n",
    "# ===============================\n",
    "\n",
    "maximums = [\"2012_01.npy\", \"2013_02.npy\", \"2013_03.npy\", \"2010_04.npy\", \"2010_05.npy\", \"2013_06.npy\", \"2013_07.npy\", \"2014_08.npy\", \"2014_09.npy\", \"2013_10.npy\", \"2014_11.npy\", \"2014_12.npy\"]\n",
    "sic_path = \"../data/datasets/post_standard/sic/\"\n",
    "\n",
    "def mask_maximum(sample, ground_truth, maximum):\n",
    "    land = np.where(np.isnan(ground_truth))\n",
    "    sample[land]= 0\n",
    "    maximum_sic = np.load(sic_path+maximum)\n",
    "    mask = np.zeros((104, 104))\n",
    "    mask[np.where(maximum_sic>0.15)]=1\n",
    "    sample = sample*mask\n",
    "    return sample[:100, :100]*100\n",
    "\n",
    "def generate_error_metrics(estimates):\n",
    "    ground_truth = np.load(\"../data/processed_data/sic/2010_01.npy\")\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    RMSE = []\n",
    "    NRMSE = []\n",
    "    NSE = []\n",
    "    MAE = []\n",
    "\n",
    "    for m in range(0, 12):\n",
    "        ice_sample = mask_maximum(estimates[0][0], ground_truth, maximums[m])\n",
    "        if m <9:\n",
    "            month = f'0{m+1}'\n",
    "        else:\n",
    "            month = f'{m+1}'\n",
    "        true_ice = mask_maximum(np.load(f'../data/datasets/post_standard/sic/2019_{month}.npy'), ground_truth, maximums[m])\n",
    "\n",
    "         # CALCULATE RMSE:\n",
    "        RMSE.append(mean_squared_error(true_ice, ice_sample, squared = False))\n",
    "\n",
    "        # CALCULATE nRMSE:\n",
    "        NRMSE.append(RMSE[m-1]/np.std(true_ice))\n",
    "\n",
    "        # CALCULATE MAE:\n",
    "        MAE.append(mean_absolute_error(true_ice, ice_sample))\n",
    "\n",
    "        # CALCULATE NSE:\n",
    "        NSE.append((1-(np.sum((true_ice-ice_sample)**2)/np.sum((true_ice-np.mean(true_ice))**2))))\n",
    "    \n",
    "    print(\"=================================================================================================================\")\n",
    "    print(f'|\\tMonth\\t|\\tRMSE\\t\\t|\\tnRMSE\\t\\t|\\tMAE\\t\\t|\\tNSE\\t\\t|')\n",
    "    print(\"=================================================================================================================\")\n",
    "    for month_count, month in enumerate(months):\n",
    "        print(f'|\\t{month}\\t|\\t{RMSE[month_count-1]:.3f}\\t\\t|\\t{NRMSE[month_count-1]:.3f}\\t\\t|\\t{MAE[month_count-1]:.3f}\\t\\t|\\t{NSE[month_count-1]:.3f}\\t\\t|')\n",
    "    print(\"=================================================================================================================\")\n",
    "    # APPLY JULIANs METRICS?\n",
    "\n",
    "    print(np.mean(np.array(MAE)))\n",
    "\n",
    "    return\n",
    "\n",
    "thickness_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_training/non_evidenced_estimates.npy\")\n",
    "baseline_predictions = np.load(f\"./models/{EXPERIMENT_ID}/post_baseline/non_evidenced_estimates.npy\")\n",
    "print(\"BASELINE\")\n",
    "generate_error_metrics(baseline_predictions)\n",
    "print(\"THICKNESS\")\n",
    "generate_error_metrics(thickness_predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
